{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egKgYaQZdFv6"
      },
      "outputs": [],
      "source": [
        "#downsampling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset (adjust the file path as needed)\n",
        "file_path = \"/content/Reviews.csv\"  # Change this to your actual file path\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "m-OomEd3eVfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns to drop\n",
        "columns_to_drop = ['Id',  'UserId', 'ProfileName','ProductId',\n",
        "                   'HelpfulnessNumerator', 'HelpfulnessDenominator',\n",
        "                   'Time', 'Summary']\n",
        "\n",
        "# Drop unnecessary columns\n",
        "dataset = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(dataset.head())\n",
        "print(dataset.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZsHO3T2ecpl",
        "outputId": "2e9e3e57-0df5-4d4f-8571-ce5272d8ff70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score                                               Text\n",
            "0      5  I have bought several of the Vitality canned d...\n",
            "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
            "2      4  This is a confection that has been around a fe...\n",
            "3      2  If you are looking for the secret ingredient i...\n",
            "4      5  Great taffy at a great price.  There was a wid...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   Score   568454 non-null  int64 \n",
            " 1   Text    568454 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 8.7+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reapply Sentiment conversion while keeping Score\n",
        "dataset['Sentiment'] = dataset['Score'].apply(lambda x: 'positive' if x >= 4 else ('negative' if x <= 2 else 'neutral'))\n",
        "\n",
        "# Confirm Score exists\n",
        "print(dataset.head())  # Check if Score is present"
      ],
      "metadata": {
        "id": "OrH5GDSPiF8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e8883f-927b-481b-fb2e-7c443333ca7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score                                               Text Sentiment\n",
            "0      5  I have bought several of the Vitality canned d...  positive\n",
            "1      1  Product arrived labeled as Jumbo Salted Peanut...  negative\n",
            "2      4  This is a confection that has been around a fe...  positive\n",
            "3      2  If you are looking for the secret ingredient i...  negative\n",
            "4      5  Great taffy at a great price.  There was a wid...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check initial class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(dataset['Sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iESwv8DOibOA",
        "outputId": "5e3b4c6a-dc59-4a9a-f7e7-2cb1b974248f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution:\n",
            "Sentiment\n",
            "positive    443777\n",
            "negative     82037\n",
            "neutral      42640\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate classes\n",
        "positive = dataset[dataset['Sentiment'] == 'positive']\n",
        "negative = dataset[dataset['Sentiment'] == 'negative']\n",
        "neutral = dataset[dataset['Sentiment'] == 'neutral']"
      ],
      "metadata": {
        "id": "XLjnIdcxeckG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[['Score', 'Sentiment','Text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap9LrTnqtMKm",
        "outputId": "29cfb5df-4457-4979-ae4b-4a4b43911b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score Sentiment                                               Text\n",
            "0      5  positive  I have bought several of the Vitality canned d...\n",
            "1      1  negative  Product arrived labeled as Jumbo Salted Peanut...\n",
            "2      4  positive  This is a confection that has been around a fe...\n",
            "3      2  negative  If you are looking for the secret ingredient i...\n",
            "4      5  positive  Great taffy at a great price.  There was a wid...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsample each class\n",
        "positive_downsampled = resample(positive, replace=False, n_samples=15000, random_state=42)\n",
        "negative_downsampled = resample(negative, replace=False, n_samples=15000, random_state=42)\n",
        "neutral_downsampled = resample(neutral, replace=False, n_samples=15000, random_state=42)\n"
      ],
      "metadata": {
        "id": "DOe8-kEVjv31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine downsampled data\n",
        "balanced_df = pd.concat([positive_downsampled, negative_downsampled, neutral_downsampled])"
      ],
      "metadata": {
        "id": "ED4iU6vikBGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "U4CATjWHkBBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(balanced_df['Sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_Y9iv1kjnr",
        "outputId": "7a0af39e-d148-400d-d197-e34e4b91689e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment\n",
            "neutral     15000\n",
            "positive    15000\n",
            "negative    15000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[['Score', 'Sentiment','Text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXDy1ZaftUZH",
        "outputId": "eb1bf1a5-7a0f-4ba7-af40-0aa0dbdd4f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score Sentiment                                               Text\n",
            "0      5  positive  I have bought several of the Vitality canned d...\n",
            "1      1  negative  Product arrived labeled as Jumbo Salted Peanut...\n",
            "2      4  positive  This is a confection that has been around a fe...\n",
            "3      2  negative  If you are looking for the secret ingredient i...\n",
            "4      5  positive  Great taffy at a great price.  There was a wid...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the balanced dataset\n",
        "balanced_df.to_csv(\"/content/balanced_dataset00.csv\", index=False)"
      ],
      "metadata": {
        "id": "ahv4lAx0km0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i366_s7c6dTy",
        "outputId": "2d011bb8-2594-456c-a5c7-136bf1cdb631",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "from wordcloud import STOPWORDS\n",
        "import contractions\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7VchNP-lg3T",
        "outputId": "dcc4a629-3a1f-45ed-edc0-a9c54ea908b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources (run only once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQVjvYN_mGWS",
        "outputId": "af665303-f862-4dd5-be6e-c94f5ec009db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/balanced_dataset00.csv\")"
      ],
      "metadata": {
        "id": "lmZFht4EmIOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\")) | set(STOPWORDS)  # Combining NLTK & WordCloud stopwords\n"
      ],
      "metadata": {
        "id": "5chm3lphmILP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle negations\n",
        "def handle_negations(text):\n",
        "    negations = {\"not\", \"no\", \"never\", \"n't\"}  # Common negation words\n",
        "    words = text.split()\n",
        "    processed_words = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        if words[i] in negations and i + 1 < len(words):  # If a negation is found\n",
        "            processed_words.append(words[i] + \"_\" + words[i + 1])  # Combine negation with the next word\n",
        "            i += 1  # Skip the next word since it's already combined\n",
        "        else:\n",
        "            processed_words.append(words[i])\n",
        "        i += 1\n",
        "\n",
        "    return \" \".join(processed_words)\n"
      ],
      "metadata": {
        "id": "6tFXs4cGmIIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function for text cleaning + negation handling\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # Lowercasing\n",
        "    text = contractions.fix(text)  # Expanding contractions (e.g., \"can't\" -> \"cannot\")\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+|\\#\\w+', '', text)  # Remove mentions (@user) and hashtags\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters & numbers\n",
        "    text = handle_negations(text)  # Handle negations\n",
        "    text = word_tokenize(text)  # Tokenization\n",
        "    text = [lemmatizer.lemmatize(word, wordnet.VERB) for word in text if word not in stop_words]  # Lemmatization & Stopword removal\n",
        "    text = \" \".join(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "LWfLFa-HmYme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing without dropping 'Score'\n",
        "df['cleaned_text'] = df['Text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Ensure 'Score' is still present\n",
        "print(df.head())  # Check if 'Score' is still there\n"
      ],
      "metadata": {
        "id": "m9McAKgOmYj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e371942-ece8-4b61-a634-14a6c0c38010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score                                               Text Sentiment  \\\n",
            "0      3  As others have noted, this jerky is chopped an...   neutral   \n",
            "1      5  My boyfriend and I LOVE this tea. Though it do...  positive   \n",
            "2      3  I wanted to buy a whey protein that didn't hav...   neutral   \n",
            "3      5  We love pop chips at our house, they are a gre...  positive   \n",
            "4      3  Both of my cats like the chicken & brown rice ...   neutral   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  others note jerky chop form greasy taste rich ...  \n",
            "1  boyfriend love tea though not_necessarily feel...  \n",
            "2  want buy whey protein not_have artificial swee...  \n",
            "3  love pop chip house great addition weight watc...  \n",
            "4  cat chicken brown rice newmans dry food flavor...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cleaned dataset\n",
        "df[['cleaned_text', 'Sentiment', 'Score']].to_csv(\"/content/cleaned_advanced_dataset(45k).csv\", index=False)"
      ],
      "metadata": {
        "id": "5Qka5flmpa9Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}